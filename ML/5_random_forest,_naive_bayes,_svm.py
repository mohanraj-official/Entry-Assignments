# -*- coding: utf-8 -*-
"""5 - Random Forest, Naive Bayes, SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYOyje2r74MqNjWzCOee8lhvoYr-QtIh

# Random Forest, Naive Bayes, SVM

## 1 - Data Preprocessing

### 1.1 - Load the Dataset
"""

import pandas as pd
import numpy as np
df = pd.read_csv("https://raw.githubusercontent.com/ArchanaInsights/Datasets/refs/heads/main/crowdfunding_campaign.csv")
print(df.info())

df.head()

"""### 1.2 - Handle Missing Values"""

df.isnull().sum()

df.describe()

num_columns = []
cat_columns = []
for i in df.select_dtypes(include = "number"):
  num_columns.append(i)
for i in df.iloc[:, 1:].select_dtypes(include = "O"):
  cat_columns.append(i)

print(f"Numerical Columns \t: {num_columns}")
print(f"Categarical Columns \t: {cat_columns}")

# Find Unique Values
for i in cat_columns:
  print(f"Unique Values for {i}\t: {list(df[i].unique())}\n")

"""### 1.3 - Encode Categorical Features"""

# Label Encoded with Binary column
df["VideoIncluded"] = df["VideoIncluded"].map({"Yes" : 1, "No" : 0})

# Label Encoded with ordinal column
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df["LaunchMonth"] = le.fit_transform(df["LaunchMonth"])

# Onehot Encoding
from sklearn.preprocessing import OneHotEncoder
onehot = OneHotEncoder()
one_hot_enc_cols = ["Category", "Country", "Currency"]
for i in one_hot_enc_cols:
  df[i] = onehot.fit_transform(df[[i]]).toarray()

dff = df.copy()
dff.drop("CampaignID", axis = 1, inplace = True)
dff.head()

"""### 1.4 - Feature Selection"""

# Select the Feature
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize = (12, 5))
plt.title("Correalation between features")
sns.heatmap(dff.corr(), annot = True, fmt = ".3f")
plt.show()

# Select K best
from sklearn.feature_selection import SelectKBest, f_classif
X = dff.drop(columns = ["IsSuccessful"])
y = df["IsSuccessful"]
selector = SelectKBest(score_func = f_classif, k = 4)
selector.fit(X, y)

top_features = X.columns[selector.get_support()]
print(f"Selected Features - :{list(top_features)}")

"""### 1.5 - Data Splitting"""

# Split the Data
from sklearn.model_selection import train_test_split
X = dff[list(top_features)]
y = df["IsSuccessful"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)
print(f"X_train Shape \t- {X_train.shape}")
print(f"X_test Shape \t- {X_test.shape}")

print(f"y_train Shape \t- {y_train.shape}")
print(f"y_test Shape \t- {y_test.shape}")

"""### 1.6 - Feature Scaling"""

# scaling the train and test data set
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)
y_train_scaled = sc.fit_transform(y_train.values.reshape(-1, 1))
y_test_scaled = sc.transform(y_test.values.reshape(-1, 1))

"""## 2 - Random Forest - Model Building and Evaluation

### 2.1 - Model Building
"""

# RANDOM FOREST MODEL
from sklearn.ensemble import RandomForestClassifier
rfcls = RandomForestClassifier(n_estimators = 50, max_depth = 3, random_state = 2)
rfcls.fit(X_train_scaled, y_train)

"""### 2.2 - Model Evaluation"""

# Evalution of Random forest classifier
from sklearn.metrics import accuracy_score
y_pred = rfcls.predict(X_test)
accuracy_score_rf = accuracy_score(y_test, y_pred)
print(f"Accuracy Score - {accuracy_score_rf}")

"""## 3 - Naive Bayes - Model Building and Evaluation

### 3.1 - Model Building
"""

#Naive Bayes implementing
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train, y_train)

"""### 3.2 - Model Evaluation"""

# GuassianNB model Evaluation
y_pred_NB = model.predict(X_test)
acc_score_Guassian_NB = accuracy_score(y_test, y_pred_NB)
print(f"Accuracy Score - {acc_score_Guassian_NB}")

# MultinomialNB model evaluation
from sklearn.naive_bayes import MultinomialNB
model_MNB = MultinomialNB()
model_MNB.fit(X_train, y_train)
y_pred_MNB = model_MNB.predict(X_test)
acc_score_Mutinomial_NB = accuracy_score(y_test, y_pred_MNB)
print(f"Accuracy Score - {acc_score_Mutinomial_NB}")

# bernoulliNB model Evaluation
from sklearn.naive_bayes import BernoulliNB
model_BNB = BernoulliNB()
model_BNB.fit(X_train, y_train)
y_pred_BNB = model_BNB.predict(X_test)
acc_score_BNB = accuracy_score(y_test, y_pred_BNB)
print(f"Accuracy Score - {acc_score_BNB}")

##After Evaluation 3 models of Naive Bayes we acn choose the highest accuracy having model GuassianNB

"""## 4 - Support Vector Machine (SVM) - Model Building and Evaluation

### 4.1 - Model Building and Evaluation
"""

# SVM building
from sklearn.svm import SVC
cls = SVC(kernel = "linear", max_iter = 1000)
cls.fit(X, y)
y_pred = cls.predict(X_test)
acc_score_svm = accuracy_score(y_test, y_pred)
print(f"Accuracy Score for 'linear' - {acc_score_svm}\n\n")

"""## 5 - Comparison and Analysis

### 5.1 - Accuracy Comparisons
"""

# Comparison Analysis
print(f"Accuracy score of random forest : {accuracy_score_rf}")
print(f"Accuracy score of Guassian Naive Bayes : {acc_score_Guassian_NB}")
print(f"Accuracy score of Multinomial Naive Bayes : {acc_score_Mutinomial_NB}")
print(f"Accuracy score of Bernoulli Naive Bayes : {acc_score_BNB}")
print(f"Accuracy score of SVM : {acc_score_svm}")

"""### 5.2 - Discussions of model
*  Gaussian Naive Bayes performed the best with an accuracy score of 0.66556
*  It is a continuous and follow a Gaussian distribution.

### 5.3 - Strength and Weakness
#### Strengths
*  Works well with continuous features.
*  Fast and quick Computational speed.
*  Requires minimal data preprocessing.

#### Weakness
*  It not hold the True in some dataset.
*  Sensitive in incorrect data.
"""